import os
import numpy as np
import pandas as pd
import torch
import torchaudio
import torch.nn as nn
from transformers import WavLMModel, Wav2Vec2FeatureExtractor
from typing import Dict, Any
import argparse

LABELS = ["FP", "RP", "RV", "RS", "PW"]
SAMPLE_RATE = 16000  # WavLM expects 16k


# ================ Model ================ #

class AcousticModel(nn.Module):
    def __init__(self):
        super(AcousticModel, self).__init__()
        self.basemodel = WavLMModel.from_pretrained("microsoft/wavlm-base")
        self.linear = nn.Linear(768, 5)

    def forward(self, x):
        # x: [B, T_samples] after feature extractor
        feats = self.basemodel.feature_extractor(x)      # [B, feature_dim, T']
        feats = feats.transpose(1, 2)                    # [B, T', feature_dim]
        feats, _ = self.basemodel.feature_projection(feats)
        emb = self.basemodel.encoder(feats, return_dict=True)[0]  # [B, T_model, 768]
        out = self.linear(emb)                           # [B, T_model, 5]
        return emb, out


# ================ Audio & Metadata Helpers ================ #

def extract_speaker_from_filename(audio_path: str) -> str:
    base = os.path.basename(audio_path)
    speaker_id = os.path.splitext(base)[0]
    return speaker_id


def load_full_audio(audio_path: str, target_sr: int = SAMPLE_RATE) -> torch.Tensor:
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"Audio file not found: {audio_path}")

    waveform, sr = torchaudio.load(audio_path)  # [channels, samples]

    # Convert to mono
    if waveform.size(0) > 1:
        waveform = waveform.mean(dim=0, keepdim=True)

    # Resample
    if sr != target_sr:
        waveform = torchaudio.functional.resample(waveform, sr, target_sr)

    return waveform  # [1, T]


def load_metadata_for_speaker(metadata_path: str, speaker_id: str) -> pd.DataFrame:
    if metadata_path.endswith(".csv"):
        meta = pd.read_csv(metadata_path)
    else:
        meta = pd.read_excel(metadata_path)

    required_cols = ["segid", "segstart", "segend"]
    for col in required_cols:
        if col not in meta.columns:
            raise ValueError(f"Metadata must contain column '{col}'")

    # Keep only segments for this speaker
    mask = meta["segid"].astype(str).str.startswith(speaker_id + "_")
    speaker_meta = meta[mask].copy()

    if speaker_meta.empty:
        raise ValueError(f"No segments found in metadata for speaker_id='{speaker_id}'")

    # Sort by segstart
    speaker_meta.sort_values(by="segstart", inplace=True)
    speaker_meta.reset_index(drop=True, inplace=True)

    return speaker_meta


# ================ Label Construction on MODEL GRID ================ #

def build_frame_labels_from_words(
    word_dir: str,
    segid: str,
    seg_start: float,
    seg_end: float,
    n_frames_model: int,
):
    """
    Build frame-level labels directly on the model's frame grid.

    - n_frames_model = T_model from the WavLM output.
    - We map frame k to a time in [0, duration] (segment-relative).
    - If that time falls inside a word [wordstart, wordend), we assign that word's labels.
    """
    csv_path = os.path.join(word_dir, f"{segid}.csv")
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Missing word-level CSV: {csv_path}")

    df = pd.read_csv(csv_path)

    for col in ["wordstart", "wordend"]:
        if col not in df.columns:
            raise ValueError(f"{csv_path} must contain '{col}' column.")
    for col in ["fp", "rp", "rv", "pw"]:
        if col not in df.columns:
            raise ValueError(f"{csv_path} must contain '{col}' column.")

    word_starts = df["wordstart"].astype(float).to_numpy()
    word_ends = df["wordend"].astype(float).to_numpy()
    fp = df["fp"].fillna(0).astype(int).to_numpy()
    rp = df["rp"].fillna(0).astype(int).to_numpy()
    rv = df["rv"].fillna(0).astype(int).to_numpy()
    pw = df["pw"].fillna(0).astype(int).to_numpy()
    rs = np.zeros_like(fp, dtype=int)  # RS not annotated

    duration = seg_end - seg_start

    # Model frame times: midpoints of equal bins over [0, duration]
    frame_times_seg = (np.arange(n_frames_model) + 0.5) * duration / n_frames_model

    # Detect whether word times are segment-relative or absolute
    max_word_end = float(np.max(word_ends)) if len(word_ends) > 0 else 0.0
    eps = 0.05  # 50 ms tolerance
    if max_word_end <= duration + eps:
        # Already relative [0, duration]
        w_starts_seg = word_starts
        w_ends_seg = word_ends
    else:
        # Absolute -> convert to segment-relative
        w_starts_seg = word_starts - seg_start
        w_ends_seg = word_ends - seg_start

    labels = np.zeros((n_frames_model, len(LABELS)), dtype=np.float32)

    for i in range(len(df)):
        w_start = w_starts_seg[i]
        w_end = w_ends_seg[i]

        # Skip words outside segment
        if w_end <= 0.0 or w_start >= duration:
            continue

        # Clamp to [0, duration]
        w_start_c = max(w_start, 0.0)
        w_end_c = min(w_end, duration)

        # Frames whose time falls inside this word interval
        mask = (frame_times_seg >= w_start_c) & (frame_times_seg < w_end_c)
        if not np.any(mask):
            continue

        lbl_vec = np.array([fp[i], rp[i], rv[i], rs[i], pw[i]], dtype=np.float32)
        labels[mask] = np.maximum(labels[mask], lbl_vec)

    return labels, frame_times_seg, df  # df is returned for word-level printing


# ================ Metrics ================ #

def compute_metrics(y_true: np.ndarray, y_logits: np.ndarray, threshold: float = 0.5) -> Dict[str, Any]:
    if y_true.size == 0:
        print("Warning: no frames to evaluate.")
        return {}

    y_true = np.asarray(y_true)
    y_logits = np.asarray(y_logits)
    y_prob = 1.0 / (1.0 + np.exp(-y_logits))
    y_pred = (y_prob >= threshold).astype(int)

    num_labels = y_true.shape[1]
    results: Dict[str, Any] = {}

    def prf1(t, p):
        t = t.astype(int)
        p = p.astype(int)
        tp = np.logical_and(t == 1, p == 1).sum()
        fn = np.logical_and(t == 1, p == 0).sum()
        fp = np.logical_and(t == 0, p == 1).sum()

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        return precision, recall, f1

    recalls = []
    f1s = []

    for i in range(num_labels):
        t = y_true[:, i]
        p = y_pred[:, i]
        precision, recall, f1 = prf1(t, p)
        results[LABELS[i]] = {"precision": precision, "recall": recall, "f1": f1}
        recalls.append(recall)
        f1s.append(f1)

    results["UAR"] = float(np.mean(recalls))
    results["macro_f1"] = float(np.mean(f1s))
    return results


# ================ Main Demo ================ #

def demo(
    audio_path: str,
    metadata_path: str,
    word_dir: str,
    weights_path: str,
    device: str = "cuda" if torch.cuda.is_available() else "cpu",
    threshold: float = 0.5,
):
    speaker_id = extract_speaker_from_filename(audio_path)
    print(f"Speaker ID inferred from audio filename: {speaker_id}")

    # 1. Load full audio
    waveform = load_full_audio(audio_path, target_sr=SAMPLE_RATE)  # [1, T]
    waveform = waveform.to(device)
    num_samples = waveform.shape[-1]
    duration_sec = num_samples / SAMPLE_RATE
    print(f"Loaded audio: {audio_path}")
    print(f"  Duration: {duration_sec:.2f} seconds, samples: {num_samples}")

    # 2. Metadata
    meta_spk = load_metadata_for_speaker(metadata_path, speaker_id)
    print(f"Found {len(meta_spk)} segments for this speaker in metadata.\n")

    # 3. Feature extractor
    feature_extractor = Wav2Vec2FeatureExtractor(
        feature_size=1,
        sampling_rate=SAMPLE_RATE,
        padding_value=0.0,
        do_normalize=True,
        return_attention_mask=False,
    )

    # 4. Model
    print(f"Loading acoustic model from: {weights_path}")
    model = AcousticModel()
    state_dict = torch.load(weights_path, map_location="cpu")
    missing, unexpected = model.load_state_dict(state_dict, strict=False)
    if missing:
        print("  Missing keys:", missing)
    if unexpected:
        print("  Unexpected keys:", unexpected)

    model.to(device)
    model.eval()
    print("Loaded finetuned acoustic model.\n")

    all_true = []
    all_logits = []

    # 5. Loop over segments
    for idx, row in meta_spk.iterrows():
        segid = str(row["segid"])
        seg_start = float(row["segstart"])
        seg_end = float(row["segend"])

        print(f"Processing segment {segid}: [{seg_start:.2f}, {seg_end:.2f}] s")

        # Slice audio
        start_sample = int(seg_start * SAMPLE_RATE)
        end_sample = int(seg_end * SAMPLE_RATE)
        seg_wave = waveform[:, start_sample:end_sample]  # [1, T_seg]

        if seg_wave.shape[-1] == 0:
            print("  -> empty segment (no samples), skipping.")
            continue

        # Feature extraction for this segment
        seg_wave_mono_cpu = seg_wave[0].detach().cpu()
        audio_feats = feature_extractor(
            seg_wave_mono_cpu,
            sampling_rate=SAMPLE_RATE
        ).input_values[0]                      # 1D numpy
        audio_feats = torch.tensor(audio_feats, dtype=torch.float32).unsqueeze(0).to(device)

        # Forward through model
        with torch.no_grad():
            emb, output = model(audio_feats)   # output: [1, T_model, 5]

        logits = output[0]                     # [T_model, 5]
        T_model = logits.shape[0]

        # Build GT on the model's frame grid
        labels, frame_times_seg, df_words = build_frame_labels_from_words(
            word_dir=word_dir,
            segid=segid,
            seg_start=seg_start,
            seg_end=seg_end,
            n_frames_model=T_model,
        )

        logits_np = logits.cpu().numpy()
        probs = 1.0 / (1.0 + np.exp(-logits_np))
        preds = (probs >= threshold).astype(int)

        all_true.append(labels)
        all_logits.append(logits_np)

        # # ---------- Frame-level print ----------
        # print("  Frame-level GT vs Predictions (first 10 frames):")
        # for i in range(min(10, T_model)):
        #     gt_str = ", ".join(f"{LABELS[j]}={labels[i, j]:.0f}" for j in range(len(LABELS)))
        #     pred_str = ", ".join(f"{LABELS[j]}={preds[i, j]:.0f}" for j in range(len(LABELS)))
        #     print(f"    frame {i:03d}: GT[{gt_str}] | Pred[{pred_str}]")

        # ---------- Word-level summary (matches CSV rows) ----------
        print("  Word-level GT vs Predictions:")
        duration = seg_end - seg_start
        for wi, word_row in df_words.iterrows():
            w_start = float(word_row["wordstart"])
            w_end = float(word_row["wordend"])

            # If for some reason they were absolute, correct them:
            if w_end > duration + 0.05:
                w_start = w_start - seg_start
                w_end = w_end - seg_start

            if w_end <= 0.0 or w_start >= duration:
                continue

            w_start_c = max(w_start, 0.0)
            w_end_c = min(w_end, duration)

            # Which frames belong to this word?
            mask = (frame_times_seg >= w_start_c) & (frame_times_seg < w_end_c)
            if not np.any(mask):
                continue

            word_gt = np.array([
                int(word_row.get("fp", 0) or 0),
                int(word_row.get("rp", 0) or 0),
                int(word_row.get("rv", 0) or 0),
                0,
                int(word_row.get("pw", 0) or 0),
            ], dtype=int)

            word_pred_probs = probs[mask].mean(axis=0)
            word_pred = (word_pred_probs >= threshold).astype(int)

            gt_str_w = ", ".join(f"{LABELS[j]}={word_gt[j]}" for j in range(len(LABELS)))
            pred_str_w = ", ".join(f"{LABELS[j]}={word_pred[j]}" for j in range(len(LABELS)))

            print(
                f"    word '{word_row.get('word','')}' "
                f"[{w_start_c:.2f},{w_end_c:.2f}]s: GT[{gt_str_w}] | Pred[{pred_str_w}]"
            )

        print()

    # 6. Global metrics
    if not all_true:
        print("No valid segments were processed; nothing to evaluate.")
        return

    y_true = np.concatenate(all_true, axis=0)   # [N_frames_total, 5]
    y_logits = np.concatenate(all_logits, axis=0)

    metrics = compute_metrics(y_true, y_logits, threshold=threshold)

    print("\n=== Overall Frame-Level Metrics for This Speaker ===")
    for lbl in LABELS:
        m = metrics[lbl]
        print(
            f"{lbl}: precision={m['precision']:.4f}, "
            f"recall={m['recall']:.4f}, "
            f"f1={m['f1']:.4f}"
        )

    print(f"\nUAR       = {metrics['UAR']:.4f}")
    print(f"macro F1  = {metrics['macro_f1']:.4f}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Demo: evaluate acoustic disfluency model on one speaker audio using segstart/segend from metadata."
    )
    parser.add_argument("--audio_path", type=str, required=True,
                        help="Path to speaker .wav file, e.g. data/audio/26f.wav")
    parser.add_argument("--metadata_path", type=str, required=True,
                        help="Path to metadata CSV (must contain segid, segstart, segend).")
    parser.add_argument("--word_dir", type=str, required=True,
                        help="Directory with per-segment CSVs, e.g. 26f_000.csv.")
    parser.add_argument("--weights_path", type=str, required=True,
                        help="Path to acoustic model weights, e.g. checkpoints/acoustic_fluencybank.pt")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--threshold", type=float, default=0.5)
    args = parser.parse_args()

    demo(
        audio_path=args.audio_path,
        metadata_path=args.metadata_path,
        word_dir=args.word_dir,
        weights_path=args.weights_path,
        device=args.device,
        threshold=args.threshold,
    )
